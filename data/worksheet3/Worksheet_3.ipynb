{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Defining the token vocabulary:\n",
    "\n",
    "We need to map each amino acid in our sequence to a different token, which is represented by a unique number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following dictionary maps each amino acid that can be part of a protein sequence to a unique integer.\n",
    "The dictionary also contains two special tokens: <PAD> and <MASK>. <PAD> is used to pad sequences that are part of the same batch \n",
    "to the same length, and <MASK> is used to mask tokens during training.\n",
    "'''\n",
    "\n",
    "token_vocabulary = { \"Y\": 0, \"A\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7, \"I\": 8, \"K\": 9, \"L\": 10,\n",
    "                    \"M\": 11, \"N\": 12, \"P\": 13, \"Q\": 14, \"R\": 15, \"S\": 16, \"T\": 17, \"V\": 18, \"W\": 19, \n",
    "                    \"<UNK>\" : 20, \"<PAD>\": 21, \"<MASK>\": 22}\n",
    "\n",
    "def tokenize_sequence(sequence, token_vocabulary):\n",
    "    \"\"\"Converts a sequence of amino acids to a list of tokens. \n",
    "    If a token is not in the vocabulary, it is replaced by the <UNK> token.\"\"\"\n",
    "    return [token_vocabulary.get(token, token_vocabulary[\"<UNK>\"]) for token in sequence]\n",
    "\n",
    "# Test the function\n",
    "sequence = \"ACDEFGHIKLMNPQRSTX*\"\n",
    "tokens = tokenize_sequence(sequence, token_vocabulary)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Define masking function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (a) Define the function mask_tokenized_sequence following the instrcutions from doctrsing of the function bewlow: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mask_tokenized_sequence(tokenized_sequence, token_vocabulary, mask_percentage=0.15):\n",
    "    \"\"\"\n",
    "    Masks a given percentage of tokens in a sequence for MLM (Maskes Language Modeling) training.\n",
    "\n",
    "    Args:\n",
    "    tokenized_sequence: List of tokens representing the input sequence\n",
    "    token_vocabulary: Dictionary mapping tokens to integers\n",
    "    mask_percentage: Percentage of tokens to mask\n",
    "\n",
    "    Returns:\n",
    "    masked_tokenized_sequence: List of input tokens with a subset of tokens masked\n",
    "    tokenized_sequence: List of input tokens without masking\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    return masked_tokenized_sequence, tokenized_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\"> Is your function working? Test your function, using the following code:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "tokenized_sequence = tokenize_sequence(\"ACDEFGHIKLMNPQRSTVWYX\", token_vocabulary)\n",
    "masked_sequence, target_sequence = mask_tokenized_sequence(tokenized_sequence, token_vocabulary, mask_percentage=0.15)\n",
    "masked_sequence, target_sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Define padding function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (b) Define the function pad_tokenized_sequences following the instrcutions from doctrsing of the function bewlow: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokenized_sequences(tokenized_sequences, max_length):\n",
    "    \"\"\"Pad sequences to the same length (max_length). \n",
    "    If sequence is longer than allowed max_length, truncate it.\n",
    "    \n",
    "    Args:\n",
    "    tokenized_sequences: List of tokenized sequences\n",
    "    max_length: Maximum length of a sequence\n",
    "\n",
    "    Returns:\n",
    "    padded_sequences: List of padded tokenized sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\"> Is your function working? Test your function, using the following code:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"ACDEFGHIKRSTVWYX\"\n",
    "tokenized_sequence = tokenize_sequence(sequence, token_vocabulary)\n",
    "padded_sequence = pad_tokenized_sequences([tokenized_sequence], 20)\n",
    "padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Combining padding and masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens_and_pad(tokenized_sequences, token_vocabulary, max_length):\n",
    "    \"\"\"Given a list of sequences, this function masks and pads the sequences and returns\n",
    "    the masked sequences and the labels for the masked tokens.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "    tokenized_sequences: List of tokenized sequences\n",
    "    token_vocabulary: Dictionary mapping tokens to integers\n",
    "    max_length: Maximum length of a sequence (allowed for the Transformer Network)\n",
    "\n",
    "    Returns:\n",
    "    padded_inputs: List of padded tokenized sequences\n",
    "    all_labels: List of labels for each token in the padded sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    all_labels = []\n",
    "    padded_inputs = []\n",
    "    for input in tokenized_sequences:\n",
    "        #padding input\n",
    "        masked_input, _ = mask_tokenized_sequence(input, token_vocabulary, mask_percentage=0.15)\n",
    "        pad_input = pad_tokenized_sequences([masked_input], max_length = max_length)\n",
    "        padded_inputs.append(pad_input)\n",
    "\n",
    "\n",
    "        #masking input\n",
    "        unmasked_indices = list(np.where(np.array(pad_input) != token_vocabulary[\"<MASK>\"]))\n",
    "        #calculate output labels\n",
    "        if len(input) >= max_length:\n",
    "            labels = np.array(input)\n",
    "        else:\n",
    "            labels = np.array(input + [token_vocabulary.get(\"<PAD>\")]*(max_length - len(input)))\n",
    "        labels[unmasked_indices] = -100\n",
    "        all_labels.append(list(labels))\n",
    "    return padded_inputs, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (c) What labels do have the masked tokens? What labels do have the tokens that are not masked? Can you guess why?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Implement batching:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a function for batching a list of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_batches(sequences, batch_size, shuffle = False):\n",
    "    if shuffle:\n",
    "        random.shuffle(sequences)\n",
    "    batches = [sequences[i:i + batch_size] for i in range(0, len(sequences), batch_size)]\n",
    "    return batches\n",
    "\n",
    "toy_sequences = ['KHCIGHNWNCDDCCTTMD',\n",
    "                    'QNIGNYLGKGXC',\n",
    "                    'DPTYSMMMFRLSFYPCCKH',\n",
    "                    'LKVMASPAXTVQSSHKEPW',\n",
    "                    'YDEPITQGMDETHWAG',\n",
    "                    'RPILVYCQXSE',\n",
    "                    'CMLIGYHRALPSGTDHP',\n",
    "                    'TVLVYVYFEVCWCVEACFT',\n",
    "                    'HLDMTHDCGQX',\n",
    "                    'KTEWCAPTMVHAEDPCG']\n",
    "\n",
    "# Create batches\n",
    "batch_size = 2\n",
    "max_input_length = 18\n",
    "batches = create_batches(toy_sequences, batch_size)\n",
    "\n",
    "# Display the first batch for illustration\n",
    "\n",
    "first_batch = batches[0]\n",
    "\n",
    "print(\"All batches for the toy dataset:\")\n",
    "print(batches)\n",
    "print(\"First batch of the toy dataset:\")\n",
    "print(first_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Implementing positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_length, d_model)\n",
    "        for pos in range(max_length):\n",
    "            for i in range(0, d_model, 2):\n",
    "                self.encoding[pos, i] = math.sin(pos / 10000 ** (2 * i / d_model))\n",
    "                self.encoding[pos, i + 1] = math.cos(pos / 10000 ** (2 * i / d_model))\n",
    "                \n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_length, batch_size, _ = x.size()\n",
    "        # Expand the encoding to match the batch size\n",
    "        encoding = self.encoding.expand(batch_size, -1, -1)\n",
    "        #reshape to shape (seq_length, batch_size, d_model)\n",
    "        encoding = encoding.permute(1, 0, 2)\n",
    "        return encoding\n",
    "\n",
    "# Test the function\n",
    "\n",
    "positional_encoding = PositionalEncoding(d_model = 12, max_length=18)\n",
    "# Create a dummy input tensor with batch size 2, sequence length 18\n",
    "input = torch.ones(18,2, 1)\n",
    "pos_encoding = positional_encoding(input)\n",
    "pos_encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (d) What kind of positional encoding are we using in the function defined above? </span>\n",
    "\n",
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (e) For the defined ProteinEncoder below, shorty describe the forward process of an input batch. </span>\n",
    "\n",
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (f) For the defined ProteinEncoder below, define the function return_representations by following the instructions from the function's docstring. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ProteinEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, dim_feedforward, num_encoder_layers, max_length):\n",
    "        super(ProteinEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout=0.1\n",
    "\n",
    "        #token embeddings and positional encoding\n",
    "        self.embedding = nn.Embedding(vocab_size, self.d_model) \n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, max_length)\n",
    "\n",
    "        #transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(self.d_model, n_heads, dim_feedforward, self.dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "        #prediction layer\n",
    "        self.prediction_layer = nn.Linear(d_model, 21)\n",
    "\n",
    "        #initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.prediction_layer.bias.data.zero_()\n",
    "        self.prediction_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.embedding(src) \n",
    "        src = self.pos_encoder(src) + src\n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = self.prediction_layer(output)\n",
    "        return output\n",
    "    \n",
    "    def return_representations(self, src, src_mask=None):\n",
    "        '''\n",
    "        This function returns all representations of all amino acids of the input sequences.\n",
    "\n",
    "        Args:\n",
    "        src: Tokenized input sequences\n",
    "        src_mask: Attention masking for the input sequences\n",
    "\n",
    "        Returns:\n",
    "        output: All representations of the input sequences of dim (max_seq_length, batch_size, d_model)\n",
    "        '''\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initializes the enocder and defines functions for training the model for one epoch and for evaluation the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "n_heads = 4\n",
    "dim_feedforward = 512\n",
    "num_encoder_layers = 4\n",
    "max_length = 512\n",
    "\n",
    "model = ProteinEncoder(vocab_size=len(token_vocabulary), d_model=d_model, n_heads=n_heads, dim_feedforward=dim_feedforward,\n",
    "                        num_encoder_layers=num_encoder_layers, max_length=max_length)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataset, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataset:\n",
    "        inputs = [tokenize_sequence(seq, token_vocabulary) for seq in batch] # Tokenize sequences\n",
    "        inputs, labels = mask_tokens_and_pad(inputs, token_vocabulary, max_length=max_length)\n",
    "        labels = torch.tensor(labels)\n",
    "        inputs = torch.tensor(inputs)\n",
    "        attention_mask = (inputs != token_vocabulary[\"<PAD>\"]).bool().view(-1, max_length)\n",
    "        inputs = inputs[:,0,:].transpose(0,1)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src=inputs, src_mask = attention_mask)\n",
    "        \n",
    "        # Adjust outputs and labels to be flat for calculating loss\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "         for batch in dataset:\n",
    "            inputs = [tokenize_sequence(seq, token_vocabulary) for seq in batch] # Tokenize sequences\n",
    "            inputs, labels = mask_tokens_and_pad(inputs, token_vocabulary, max_length=max_length)\n",
    "            labels = torch.tensor(labels)\n",
    "            inputs = torch.tensor(inputs)\n",
    "            attention_mask = (inputs != token_vocabulary[\"<PAD>\"]).bool().view(-1, max_length)\n",
    "            inputs = inputs[:,0,:].transpose(0,1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, src_mask = attention_mask)\n",
    "            loss = loss_fn(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (g) Load the training, validation, and test datasets from the optmal pH prediction task from Worksheet 1. Store all training sequences in train_sequences, the validation sequences in val_sequences, and the test sequences in test_sequences. For sequences that are longer than 511 amino acids, only use the first 511 amino acids: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (h) Train the Protein encoder on the training sequences for at least 5 epochs. If you cannot train the model on your own PC try to reduce the model size, batch size and or maximum sequence length. Alternatively, you can run the training on the HPC. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = create_batches(val_sequences, batch_size=8)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset = create_batches(train_sequences, batch_size=8, shuffle = True)\n",
    "    train_loss = train_one_epoch(model, train_dataset, optimizer, loss_fn)\n",
    "    val_loss = evaluate(model, val_dataset, loss_fn)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (i) Define a function that calculates for a given protein amino acid sequence a single numerical representation. This representation should be the element-wise mean of all amino acid representations. Test this function for the first sequence of train_sequences </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">Exercise 3.1 (j) How many learnable parameters are in the transformer network? How many trainable parameters are in the atention heads, feedforward blocks, embeddings layer and the final prediction layer? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_for_Molecules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
