{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing ESM-2 embeddings with internet access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm # install via pip install fair-esm\n",
    "import numpy as np\n",
    "\n",
    "#when you call this model the following function for the first time, this will download the model from the internet\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval();  \n",
    "\n",
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#This function takes a protein sequence and returns the mean-embedding of the protein\n",
    "def get_esm_embedding(model, alphabet, sequence):\n",
    "    sequences = [(\"P\", sequence)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(sequences)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens.to(device), repr_layers=[6], return_contacts=False)\n",
    "    token_representations = results[\"representations\"][6].cpu().numpy()\n",
    "    #we don't want to use the first and last token:\n",
    "    protein_representations = np.mean(token_representations[0,1:-1,:], axis=0)\n",
    "    return protein_representations\n",
    "\n",
    "#example\n",
    "sequence = \"MAGLQKQK\"\n",
    "embedding = get_esm_embedding(model, alphabet, sequence)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing ESM-2 emebddings without internet access (on HPC):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You first need to download the model paramaters of the ESM-2 model (\"esm2_t6_8M_UR50D-contact-regression.pt\", \"esm2_t6_8M_UR50D.pt\") from data/worksheet4. You need to store those files on the HPC. The following code shows you how to load the model that is stored on a local file without downloading it from the internet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.load_model_and_alphabet_local(model_location = \"/path_to_parameter_weights/esm2_t6_8M_UR50D.pt\")\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code is identical to the code above for the downloaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#This function takes a protein sequence and returns the mean-embedding of the protein\n",
    "def get_esm_embedding(model, alphabet, sequence):\n",
    "    sequences = [(\"P\", sequence)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(sequences)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens.to(device), repr_layers=[6], return_contacts=False)\n",
    "    token_representations = results[\"representations\"][6].cpu().numpy()\n",
    "    #we don't want to use the first and last token:\n",
    "    protein_representations = np.mean(token_representations[0,1:-1,:], axis=0)\n",
    "    return protein_representations\n",
    "\n",
    "\n",
    "embedding = get_esm_embedding(model, alphabet, sequence)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_for_Molecules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
